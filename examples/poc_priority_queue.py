"""
Production-Level Priority Queue Implementation Comparison

This POC compares different priority queue implementations to inform ModelMora's
task scheduling architecture decision.

Queue Implementations Tested:
1. heapq (stdlib, lightweight, not thread-safe)
2. queue.PriorityQueue (stdlib, thread-safe, blocking)
3. asyncio.PriorityQueue (stdlib, async-native, coroutine-safe)
4. multiprocessing.Queue with heapq (process-safe, for worker pools)

Test Scenarios:
1. Single-threaded enqueue/dequeue performance
2. Multi-threaded concurrent access
3. Async concurrent access (asyncio)
4. Multi-process worker pool simulation
5. Priority ordering correctness under load
6. Memory overhead comparison

Metrics Tracked:
- Enqueue latency (Œºs)
- Dequeue latency (Œºs)
- Throughput (ops/sec)
- Memory overhead (MB)
- Priority order correctness (%)
- Concurrent access safety

Architecture Decision Points:
- Which queue for single-node async API server?
- Which queue for multi-process worker coordination?
- Trade-offs: performance vs safety vs complexity
- Scalability to 1000s of requests/sec
"""

import asyncio
import heapq
import multiprocessing as mp
import queue
import sys
import threading
import time
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from enum import IntEnum
from typing import Any, Dict, List, Optional, Tuple
from uuid import uuid4

try:
    import psutil

    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False


# ============================================================================
# Data Models
# ============================================================================


class Priority(IntEnum):
    """Request priority levels (lower number = higher priority)"""

    CRITICAL = 1  # SLA-bound, real-time inference
    HIGH = 2  # User-facing, interactive
    NORMAL = 3  # Batch processing, standard
    LOW = 4  # Background tasks, can be delayed


@dataclass(order=True)
class Task:
    """Priority queue task item"""

    priority: int
    timestamp: float = field(compare=True)
    task_id: str = field(default_factory=lambda: str(uuid4()), compare=False)
    model_id: str = field(default="", compare=False)
    payload: Any = field(default=None, compare=False)

    def __repr__(self):
        return f"Task(priority={self.priority}, id={self.task_id[:8]}, model={self.model_id})"


@dataclass
class QueueMetrics:
    """Performance metrics for a queue implementation"""

    queue_type: str
    enqueue_latency_us: List[float] = field(default_factory=list)
    dequeue_latency_us: List[float] = field(default_factory=list)
    total_enqueue_time_ms: float = 0.0
    total_dequeue_time_ms: float = 0.0
    throughput_ops_per_sec: float = 0.0
    memory_overhead_mb: float = 0.0
    priority_violations: int = 0
    total_operations: int = 0
    errors: int = 0

    @property
    def avg_enqueue_latency_us(self) -> float:
        return sum(self.enqueue_latency_us) / len(self.enqueue_latency_us) if self.enqueue_latency_us else 0.0

    @property
    def avg_dequeue_latency_us(self) -> float:
        return sum(self.dequeue_latency_us) / len(self.dequeue_latency_us) if self.dequeue_latency_us else 0.0

    @property
    def p99_enqueue_latency_us(self) -> float:
        if not self.enqueue_latency_us:
            return 0.0
        sorted_latencies = sorted(self.enqueue_latency_us)
        idx = int(len(sorted_latencies) * 0.99)
        return sorted_latencies[idx]

    @property
    def p99_dequeue_latency_us(self) -> float:
        if not self.dequeue_latency_us:
            return 0.0
        sorted_latencies = sorted(self.dequeue_latency_us)
        idx = int(len(sorted_latencies) * 0.99)
        return sorted_latencies[idx]

    def __str__(self):
        return (
            f"{self.queue_type}:\n"
            f"  Avg enqueue: {self.avg_enqueue_latency_us:.1f}Œºs (p99: {self.p99_enqueue_latency_us:.1f}Œºs)\n"
            f"  Avg dequeue: {self.avg_dequeue_latency_us:.1f}Œºs (p99: {self.p99_dequeue_latency_us:.1f}Œºs)\n"
            f"  Throughput: {self.throughput_ops_per_sec:.0f} ops/sec\n"
            f"  Memory: {self.memory_overhead_mb:.2f}MB\n"
            f"  Priority violations: {self.priority_violations}/{self.total_operations}\n"
            f"  Errors: {self.errors}"
        )


# ============================================================================
# Queue Implementations
# ============================================================================


class HeapQueueWrapper:
    """Thread-unsafe heapq wrapper (baseline performance)"""

    def __init__(self):
        self.heap: List[Task] = []

    def put(self, task: Task):
        heapq.heappush(self.heap, task)

    def get(self) -> Task:
        return heapq.heappop(self.heap)

    def qsize(self) -> int:
        return len(self.heap)

    def empty(self) -> bool:
        return len(self.heap) == 0


class ThreadSafeHeapQueue:
    """Thread-safe heapq with explicit locking"""

    def __init__(self):
        self.heap: List[Task] = []
        self.lock = threading.Lock()

    def put(self, task: Task):
        with self.lock:
            heapq.heappush(self.heap, task)

    def get(self) -> Task:
        with self.lock:
            return heapq.heappop(self.heap)

    def qsize(self) -> int:
        with self.lock:
            return len(self.heap)

    def empty(self) -> bool:
        with self.lock:
            return len(self.heap) == 0


# ============================================================================
# Benchmark Implementation
# ============================================================================


class PriorityQueueBenchmark:
    """Production-level priority queue comparison"""

    def __init__(self, num_tasks: int = 10000):
        self.num_tasks = num_tasks
        self.results: Dict[str, QueueMetrics] = {}

    def generate_tasks(self, count: int, randomize_priority: bool = True) -> List[Task]:
        """Generate test tasks with varying priorities"""
        tasks = []
        priorities = [Priority.CRITICAL, Priority.HIGH, Priority.NORMAL, Priority.LOW]

        for i in range(count):
            if randomize_priority:
                import random

                priority = random.choice(priorities)
            else:
                # Distribute evenly
                priority = priorities[i % len(priorities)]

            task = Task(
                priority=priority.value,
                timestamp=time.time(),
                model_id=f"model_{i % 5}",  # 5 different models
                payload={"data": f"payload_{i}"},
            )
            tasks.append(task)

        return tasks

    def verify_priority_order(self, dequeued_tasks: List[Task]) -> int:
        """Count priority ordering violations"""
        violations = 0
        for i in range(1, len(dequeued_tasks)):
            prev = dequeued_tasks[i - 1]
            curr = dequeued_tasks[i]

            # Priority should be monotonically non-decreasing
            # If same priority, timestamp should be non-decreasing (FIFO within priority)
            if prev.priority > curr.priority:
                violations += 1
            elif prev.priority == curr.priority and prev.timestamp > curr.timestamp:
                violations += 1

        return violations

    # ========================================================================
    # Test 1: Single-threaded performance (baseline)
    # ========================================================================

    def benchmark_heapq(self) -> QueueMetrics:
        """Benchmark stdlib heapq (not thread-safe)"""

        print("\n" + "=" * 70)
        print("TEST: heapq (stdlib, single-threaded)")
        print("=" * 70)

        metrics = QueueMetrics(queue_type="heapq")
        q = HeapQueueWrapper()

        tasks = self.generate_tasks(self.num_tasks)

        # Enqueue
        print(f"Enqueuing {self.num_tasks} tasks...")
        start = time.perf_counter()

        for task in tasks:
            enqueue_start = time.perf_counter()
            q.put(task)
            enqueue_time_us = (time.perf_counter() - enqueue_start) * 1_000_000
            metrics.enqueue_latency_us.append(enqueue_time_us)

        metrics.total_enqueue_time_ms = (time.perf_counter() - start) * 1000

        # Dequeue
        print(f"Dequeuing {self.num_tasks} tasks...")
        dequeued = []
        start = time.perf_counter()

        while not q.empty():
            dequeue_start = time.perf_counter()
            task = q.get()
            dequeue_time_us = (time.perf_counter() - dequeue_start) * 1_000_000
            metrics.dequeue_latency_us.append(dequeue_time_us)
            dequeued.append(task)

        metrics.total_dequeue_time_ms = (time.perf_counter() - start) * 1000

        # Calculate metrics
        total_time_sec = (metrics.total_enqueue_time_ms + metrics.total_dequeue_time_ms) / 1000
        metrics.throughput_ops_per_sec = (self.num_tasks * 2) / total_time_sec  # enqueue + dequeue
        metrics.total_operations = self.num_tasks
        metrics.priority_violations = self.verify_priority_order(dequeued)

        # Memory (approximate)
        if PSUTIL_AVAILABLE:
            import os

            process = psutil.Process(os.getpid())
            metrics.memory_overhead_mb = process.memory_info().rss / (1024 * 1024)

        print(f"‚úÖ Complete: {metrics}")

        self.results["heapq"] = metrics
        return metrics

    def benchmark_queue_priorityqueue(self) -> QueueMetrics:
        """Benchmark queue.PriorityQueue (thread-safe, blocking)"""

        print("\n" + "=" * 70)
        print("TEST: queue.PriorityQueue (stdlib, thread-safe)")
        print("=" * 70)

        metrics = QueueMetrics(queue_type="queue.PriorityQueue")
        q = queue.PriorityQueue()

        tasks = self.generate_tasks(self.num_tasks)

        # Enqueue
        print(f"Enqueuing {self.num_tasks} tasks...")
        start = time.perf_counter()

        for task in tasks:
            enqueue_start = time.perf_counter()
            q.put(task)
            enqueue_time_us = (time.perf_counter() - enqueue_start) * 1_000_000
            metrics.enqueue_latency_us.append(enqueue_time_us)

        metrics.total_enqueue_time_ms = (time.perf_counter() - start) * 1000

        # Dequeue
        print(f"Dequeuing {self.num_tasks} tasks...")
        dequeued = []
        start = time.perf_counter()

        while not q.empty():
            dequeue_start = time.perf_counter()
            task = q.get()
            dequeue_time_us = (time.perf_counter() - dequeue_start) * 1_000_000
            metrics.dequeue_latency_us.append(dequeue_time_us)
            dequeued.append(task)

        metrics.total_dequeue_time_ms = (time.perf_counter() - start) * 1000

        # Calculate metrics
        total_time_sec = (metrics.total_enqueue_time_ms + metrics.total_dequeue_time_ms) / 1000
        metrics.throughput_ops_per_sec = (self.num_tasks * 2) / total_time_sec
        metrics.total_operations = self.num_tasks
        metrics.priority_violations = self.verify_priority_order(dequeued)

        print(f"‚úÖ Complete: {metrics}")

        self.results["queue.PriorityQueue"] = metrics
        return metrics

    async def benchmark_asyncio_priorityqueue(self) -> QueueMetrics:
        """Benchmark asyncio.PriorityQueue (async-native)"""

        print("\n" + "=" * 70)
        print("TEST: asyncio.PriorityQueue (stdlib, async)")
        print("=" * 70)

        metrics = QueueMetrics(queue_type="asyncio.PriorityQueue")
        q = asyncio.PriorityQueue()

        tasks = self.generate_tasks(self.num_tasks)

        # Enqueue
        print(f"Enqueuing {self.num_tasks} tasks...")
        start = time.perf_counter()

        for task in tasks:
            enqueue_start = time.perf_counter()
            await q.put(task)
            enqueue_time_us = (time.perf_counter() - enqueue_start) * 1_000_000
            metrics.enqueue_latency_us.append(enqueue_time_us)

        metrics.total_enqueue_time_ms = (time.perf_counter() - start) * 1000

        # Dequeue
        print(f"Dequeuing {self.num_tasks} tasks...")
        dequeued = []
        start = time.perf_counter()

        while not q.empty():
            dequeue_start = time.perf_counter()
            task = await q.get()
            dequeue_time_us = (time.perf_counter() - dequeue_start) * 1_000_000
            metrics.dequeue_latency_us.append(dequeue_time_us)
            dequeued.append(task)

        metrics.total_dequeue_time_ms = (time.perf_counter() - start) * 1000

        # Calculate metrics
        total_time_sec = (metrics.total_enqueue_time_ms + metrics.total_dequeue_time_ms) / 1000
        metrics.throughput_ops_per_sec = (self.num_tasks * 2) / total_time_sec
        metrics.total_operations = self.num_tasks
        metrics.priority_violations = self.verify_priority_order(dequeued)

        print(f"‚úÖ Complete: {metrics}")

        self.results["asyncio.PriorityQueue"] = metrics
        return metrics

    # ========================================================================
    # Test 2: Multi-threaded concurrent access
    # ========================================================================

    def benchmark_concurrent_threads(self, num_producers: int = 5, num_consumers: int = 5) -> QueueMetrics:
        """Benchmark queue.PriorityQueue with concurrent threads"""

        print("\n" + "=" * 70)
        print(f"TEST: Multi-threaded ({num_producers} producers, {num_consumers} consumers)")
        print("=" * 70)

        metrics = QueueMetrics(queue_type="queue.PriorityQueue (concurrent)")
        q = queue.PriorityQueue()

        tasks_per_producer = self.num_tasks // num_producers
        all_tasks = self.generate_tasks(self.num_tasks)

        dequeued_tasks = []
        dequeue_lock = threading.Lock()

        def producer(tasks_chunk: List[Task]):
            """Producer thread"""
            for task in tasks_chunk:
                enqueue_start = time.perf_counter()
                q.put(task)
                enqueue_time_us = (time.perf_counter() - enqueue_start) * 1_000_000
                with dequeue_lock:
                    metrics.enqueue_latency_us.append(enqueue_time_us)

        def consumer(num_items: int):
            """Consumer thread"""
            consumed = 0
            while consumed < num_items:
                try:
                    dequeue_start = time.perf_counter()
                    task = q.get(timeout=1.0)
                    dequeue_time_us = (time.perf_counter() - dequeue_start) * 1_000_000

                    with dequeue_lock:
                        metrics.dequeue_latency_us.append(dequeue_time_us)
                        dequeued_tasks.append(task)

                    consumed += 1
                except queue.Empty:
                    break

        # Start benchmark
        print(f"Starting {num_producers} producers and {num_consumers} consumers...")
        start = time.perf_counter()

        # Start producers
        producer_threads = []
        for i in range(num_producers):
            chunk_start = i * tasks_per_producer
            chunk_end = chunk_start + tasks_per_producer
            chunk = all_tasks[chunk_start:chunk_end]
            t = threading.Thread(target=producer, args=(chunk,))
            t.start()
            producer_threads.append(t)

        # Start consumers
        items_per_consumer = self.num_tasks // num_consumers
        consumer_threads = []
        for _ in range(num_consumers):
            t = threading.Thread(target=consumer, args=(items_per_consumer,))
            t.start()
            consumer_threads.append(t)

        # Wait for all threads
        for t in producer_threads:
            t.join()
        for t in consumer_threads:
            t.join()

        total_time_ms = (time.perf_counter() - start) * 1000

        # Calculate metrics
        metrics.throughput_ops_per_sec = (self.num_tasks * 2) / (total_time_ms / 1000)
        metrics.total_operations = self.num_tasks
        metrics.priority_violations = self.verify_priority_order(dequeued_tasks)

        print(f"‚úÖ Complete in {total_time_ms:.1f}ms")
        print(f"   {metrics}")

        self.results["concurrent_threads"] = metrics
        return metrics

    # ========================================================================
    # Test 3: Async concurrent access
    # ========================================================================

    async def benchmark_concurrent_async(self, num_producers: int = 5, num_consumers: int = 5) -> QueueMetrics:
        """Benchmark asyncio.PriorityQueue with concurrent coroutines"""

        print("\n" + "=" * 70)
        print(f"TEST: Async concurrent ({num_producers} producers, {num_consumers} consumers)")
        print("=" * 70)

        metrics = QueueMetrics(queue_type="asyncio.PriorityQueue (concurrent)")
        q = asyncio.PriorityQueue()

        tasks_per_producer = self.num_tasks // num_producers
        all_tasks = self.generate_tasks(self.num_tasks)

        dequeued_tasks = []

        async def producer(tasks_chunk: List[Task]):
            """Producer coroutine"""
            for task in tasks_chunk:
                enqueue_start = time.perf_counter()
                await q.put(task)
                enqueue_time_us = (time.perf_counter() - enqueue_start) * 1_000_000
                metrics.enqueue_latency_us.append(enqueue_time_us)

        async def consumer(num_items: int):
            """Consumer coroutine"""
            consumed = 0
            while consumed < num_items:
                try:
                    dequeue_start = time.perf_counter()
                    task = await asyncio.wait_for(q.get(), timeout=1.0)
                    dequeue_time_us = (time.perf_counter() - dequeue_start) * 1_000_000

                    metrics.dequeue_latency_us.append(dequeue_time_us)
                    dequeued_tasks.append(task)

                    consumed += 1
                except asyncio.TimeoutError:
                    break

        # Start benchmark
        print(f"Starting {num_producers} producers and {num_consumers} consumers...")
        start = time.perf_counter()

        # Create producer coroutines
        producer_coros = []
        for i in range(num_producers):
            chunk_start = i * tasks_per_producer
            chunk_end = chunk_start + tasks_per_producer
            chunk = all_tasks[chunk_start:chunk_end]
            producer_coros.append(producer(chunk))

        # Create consumer coroutines
        items_per_consumer = self.num_tasks // num_consumers
        consumer_coros = [consumer(items_per_consumer) for _ in range(num_consumers)]

        # Run all concurrently
        await asyncio.gather(*producer_coros, *consumer_coros)

        total_time_ms = (time.perf_counter() - start) * 1000

        # Calculate metrics
        metrics.throughput_ops_per_sec = (self.num_tasks * 2) / (total_time_ms / 1000)
        metrics.total_operations = self.num_tasks
        metrics.priority_violations = self.verify_priority_order(dequeued_tasks)

        print(f"‚úÖ Complete in {total_time_ms:.1f}ms")
        print(f"   {metrics}")

        self.results["concurrent_async"] = metrics
        return metrics

    # ========================================================================
    # Summary & Recommendations
    # ========================================================================

    def print_comparison(self):
        """Print detailed comparison and recommendations"""

        print("\n" + "=" * 70)
        print("COMPARISON: Priority Queue Implementations")
        print("=" * 70)

        if not self.results:
            print("‚ùå No results to compare")
            return

        # Sort by throughput
        sorted_results = sorted(self.results.items(), key=lambda x: x[1].throughput_ops_per_sec, reverse=True)

        print("\nüìä Throughput Ranking (ops/sec):")
        print("-" * 70)
        for i, (name, metrics) in enumerate(sorted_results, 1):
            print(f"{i}. {name:30s} {metrics.throughput_ops_per_sec:>12,.0f} ops/sec")

        print("\n‚ö° Latency Comparison (avg Œºs):")
        print("-" * 70)
        print(f"{'Queue Type':<30s} {'Enqueue':>12s} {'Dequeue':>12s} {'P99 Enqueue':>15s} {'P99 Dequeue':>15s}")
        print("-" * 70)

        for name, metrics in sorted_results:
            print(
                f"{name:<30s} "
                f"{metrics.avg_enqueue_latency_us:>12.1f} "
                f"{metrics.avg_dequeue_latency_us:>12.1f} "
                f"{metrics.p99_enqueue_latency_us:>15.1f} "
                f"{metrics.p99_dequeue_latency_us:>15.1f}"
            )

        print("\nüéØ Priority Order Correctness:")
        print("-" * 70)
        for name, metrics in self.results.items():
            correctness = (
                (metrics.total_operations - metrics.priority_violations) / metrics.total_operations * 100
                if metrics.total_operations > 0
                else 0
            )
            status = "‚úÖ" if correctness == 100.0 else "‚ö†Ô∏è"
            print(f"{status} {name:30s} {correctness:>6.2f}% ({metrics.priority_violations} violations)")

        # Recommendations
        print("\n" + "=" * 70)
        print("RECOMMENDATIONS FOR MODELMORA")
        print("=" * 70)

        # Find fastest for different scenarios
        fastest_overall = sorted_results[0]
        heapq_metrics = self.results.get("heapq")
        asyncio_metrics = self.results.get("asyncio.PriorityQueue")
        queue_metrics = self.results.get("queue.PriorityQueue")
        concurrent_async_metrics = self.results.get("concurrent_async")

        print("\n1. Single-Node Async API Server:")
        if asyncio_metrics:
            print(f"   ‚úÖ Use: asyncio.PriorityQueue")
            print(f"   Throughput: {asyncio_metrics.throughput_ops_per_sec:,.0f} ops/sec")
            print(f"   Avg latency: {asyncio_metrics.avg_enqueue_latency_us:.1f}Œºs enqueue")
            print(f"   üí° Native async support, perfect for FastAPI/gRPC async handlers")
        else:
            print(f"   ‚ö†Ô∏è  asyncio.PriorityQueue not tested")

        print("\n2. Multi-threaded Workers:")
        if queue_metrics:
            print(f"   ‚úÖ Use: queue.PriorityQueue")
            print(f"   Throughput: {queue_metrics.throughput_ops_per_sec:,.0f} ops/sec")
            print(f"   üí° Thread-safe, blocking semantics, stdlib")
        else:
            print(f"   ‚ö†Ô∏è  queue.PriorityQueue not tested")

        print("\n3. Performance-Critical Single-Threaded:")
        if heapq_metrics:
            print(f"   ‚úÖ Use: heapq (with manual locking if needed)")
            print(f"   Throughput: {heapq_metrics.throughput_ops_per_sec:,.0f} ops/sec")
            print(f"   Avg latency: {heapq_metrics.avg_enqueue_latency_us:.1f}Œºs enqueue")
            print(f"   üí° Fastest raw performance, minimal overhead")
        else:
            print(f"   ‚ö†Ô∏è  heapq not tested")

        print("\n4. Distributed Multi-Node (Future):")
        print(f"   üìã Use: Redis sorted sets or dedicated message queue")
        print(f"   üí° For Phase 4 multi-node coordination")

        # Latency targets
        print("\n5. Latency Analysis:")
        avg_enqueue = sum(m.avg_enqueue_latency_us for m in self.results.values()) / len(self.results)

        if avg_enqueue < 10:
            print(f"   ‚úÖ Excellent: {avg_enqueue:.1f}Œºs avg enqueue latency")
            print(f"   üí° Negligible queue overhead (<50ms target)")
        elif avg_enqueue < 100:
            print(f"   ‚úÖ Good: {avg_enqueue:.1f}Œºs avg enqueue latency")
            print(f"   üí° Queue overhead acceptable for ModelMora")
        else:
            print(f"   ‚ö†Ô∏è  Moderate: {avg_enqueue:.1f}Œºs avg enqueue latency")
            print(f"   üí° May need optimization for high-throughput scenarios")

        print("\n" + "=" * 70)
        print("ARCHITECTURE DECISION")
        print("=" * 70)

        print("\nFor ModelMora MVP (Phase 1):")
        print("  ‚úÖ Use asyncio.PriorityQueue for request queue")
        print("  ‚úÖ Integrate with FastAPI async endpoints")
        print("  ‚úÖ Priority levels: CRITICAL, HIGH, NORMAL, LOW")
        print("  ‚úÖ FIFO within same priority (timestamp ordering)")

        print("\nExpected Performance:")
        if asyncio_metrics:
            print(f"  ‚Ä¢ Handle {asyncio_metrics.throughput_ops_per_sec:,.0f} requests/sec")
            print(f"  ‚Ä¢ Queue latency: <{asyncio_metrics.avg_enqueue_latency_us:.0f}Œºs")
            print(f"  ‚Ä¢ Well within <50ms target")


# ============================================================================
# Main Test Runner
# ============================================================================


def main():
    """Run comprehensive priority queue benchmark"""

    print("=" * 70)
    print("ModelMora: Priority Queue Implementation Comparison")
    print("=" * 70)

    # Check dependencies
    print("\nDependency Check:")
    print(f"  psutil: {'‚úÖ' if PSUTIL_AVAILABLE else '‚ùå'}")

    # Configuration
    num_tasks = 10000
    print(f"\nConfiguration:")
    print(f"  Test tasks: {num_tasks:,}")
    print(f"  Priority levels: {len(Priority)} (CRITICAL, HIGH, NORMAL, LOW)")

    # Initialize benchmark
    benchmark = PriorityQueueBenchmark(num_tasks=num_tasks)

    print("\n" + "=" * 70)
    print("RUNNING BENCHMARKS")
    print("=" * 70)

    # Test 1: Single-threaded baseline
    print("\nüî¨ Test 1: Single-threaded Performance (Baseline)")
    benchmark.benchmark_heapq()
    benchmark.benchmark_queue_priorityqueue()

    # Test 2: Async
    print("\nüî¨ Test 2: Async Performance")
    asyncio.run(benchmark.benchmark_asyncio_priorityqueue())

    # Test 3: Multi-threaded concurrent
    print("\nüî¨ Test 3: Multi-threaded Concurrent Access")
    benchmark.benchmark_concurrent_threads(num_producers=5, num_consumers=5)

    # Test 4: Async concurrent
    print("\nüî¨ Test 4: Async Concurrent Access")
    asyncio.run(benchmark.benchmark_concurrent_async(num_producers=5, num_consumers=5))

    # Print comparison
    benchmark.print_comparison()

    print("\n" + "=" * 70)
    print("Benchmark Complete")
    print("=" * 70)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Benchmark interrupted")
    except Exception as e:
        print(f"\n\n‚ùå Error: {e}")
        import traceback

        traceback.print_exc()
